{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1efaf13e-9440-4164-af75-addd34bc975d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers datasets peft accelerate torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "456c6f2b-597f-4b60-a947-82c8527e1581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using torch dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer, # <--- Using standard Trainer\n",
    "    pipeline,\n",
    "    logging,\n",
    "    DataCollatorForLanguageModeling,\n",
    ")\n",
    "from peft import LoraConfig, PeftModel, get_peft_model\n",
    "# Note: prepare_model_for_kbit_training is for bitsandbytes, removed\n",
    "# from trl import SFTTrainer # <-- Removed SFTTrainer import\n",
    "\n",
    "# --- Configuration ---\n",
    "# Model\n",
    "base_model_name = \"Qwen/Qwen3-1.7B\"\n",
    "new_model_name = \"qwen3-1.7b-linear-algebra-coder-lora-stdtrainer\" # Updated name\n",
    "\n",
    "# Dataset\n",
    "dataset_path = \"final_dataset_no_comments (1).json\" # Assumes EOS token was added\n",
    "\n",
    "# LoRA Config\n",
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.05\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "\n",
    "# Training Config\n",
    "output_dir = \"./qwen1.8b-lora-stdtrainer-results\" # Updated output dir\n",
    "num_train_epochs = 3\n",
    "per_device_train_batch_size = 1 # START LOW for LoRA without quantization\n",
    "per_device_eval_batch_size = 1\n",
    "gradient_accumulation_steps = 16 # Adjust effective batch size\n",
    "gradient_checkpointing = True\n",
    "optim = \"adamw_torch\"\n",
    "save_strategy = \"steps\" # Use save_strategy with Trainer\n",
    "save_steps = 100\n",
    "logging_steps = 10\n",
    "learning_rate = 1e-4\n",
    "weight_decay = 0.01\n",
    "fp16 = False # Set only one to True\n",
    "bf16 = True  # Assuming Ampere+ GPU\n",
    "max_grad_norm = 0.3\n",
    "max_steps = -1\n",
    "warmup_ratio = 0.03\n",
    "lr_scheduler_type = \"cosine\"\n",
    "group_by_length = True\n",
    "evaluation_strategy = \"steps\" # Standard Trainer uses this\n",
    "eval_steps = 100\n",
    "save_total_limit = 2\n",
    "load_best_model_at_end = True\n",
    "metric_for_best_model=\"eval_loss\"\n",
    "greater_is_better=False\n",
    "report_to = \"tensorboard\"\n",
    "\n",
    "# Other\n",
    "seed = 42\n",
    "max_seq_length = 1024 # Max sequence length for tokenization\n",
    "\n",
    "# Set PYTORCH_CUDA_ALLOC_CONF if needed\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# --- Determine Torch Dtype ---\n",
    "torch_dtype = torch.bfloat16 if bf16 else (torch.float16 if fp16 else torch.float32)\n",
    "print(f\"Using torch dtype: {torch_dtype}\")\n",
    "\n",
    "# Check for CUDA availability\n",
    "if not torch.cuda.is_available():\n",
    "    print(\"Warning: CUDA not available, training will be very slow on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec7b68c4-0c04-4088-8a12-693ad9ab254e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from final_dataset_no_comments (1).json...\n",
      "Loaded 6804 examples.\n",
      "Training samples: 6123, Validation samples: 681\n",
      "Dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 6123\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 681\n",
      "    })\n",
      "})\n",
      "\n",
      "Loading tokenizer for Qwen/Qwen3-1.7B...\n",
      "\n",
      "Tokenizing and masking dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bedbb616d564f46909252e4b448f80d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6123 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cabd000a77d421a9ffd5d2d8dee2182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/681 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset structure after tokenization/masking:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 6123\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask', 'labels'],\n",
      "        num_rows: 681\n",
      "    })\n",
      "})\n",
      "\n",
      "Sample tokenized data point:\n",
      "Input IDs length: 73\n",
      "Labels length: 73\n",
      "Input IDs sample: [28468, 264, 220, 18, 87, 18, 87, 18, 15626, 448, 476, 4587, 2492, 34254, 3156, 279, 4843, 8024, 624, 474, 8591, 438, 2595, 198, 1499, 28090, 37732, 1159, 29199, 198, 6199, 7829, 36325, 7, 15, 340, 82, 37414, 284, 508, 2364, 9900, 7829, 15506, 7, 18, 11, 220, 18, 593, 369, 716, 304, 2088, 7, 18, 5563, 46111, 284, 2595]\n",
      "Labels sample: [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 474, 8591, 438, 2595, 198, 1499, 28090, 37732, 1159, 29199, 198, 6199, 7829, 36325, 7, 15, 340, 82, 37414, 284, 508, 2364, 9900, 7829, 15506, 7, 18, 11, 220, 18, 593, 369, 716, 304, 2088, 7, 18, 5563, 46111, 284, 2595]\n"
     ]
    }
   ],
   "source": [
    "# --- Load Data ---\n",
    "print(f\"Loading dataset from {dataset_path}...\")\n",
    "try:\n",
    "    with open(dataset_path, \"r\", encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Dataset file '{dataset_path}' not found.\")\n",
    "    exit()\n",
    "except json.JSONDecodeError:\n",
    "    print(f\"Error: Could not decode JSON from '{dataset_path}'.\")\n",
    "    exit()\n",
    "print(f\"Loaded {len(data)} examples.\")\n",
    "\n",
    "# --- Train/Validation Split ---\n",
    "train_data, val_data = train_test_split(data, test_size=0.1, random_state=seed)\n",
    "print(f\"Training samples: {len(train_data)}, Validation samples: {len(val_data)}\")\n",
    "\n",
    "# --- Convert to Hugging Face Dataset ---\n",
    "hf_dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_data),\n",
    "    \"validation\": Dataset.from_list(val_data)\n",
    "})\n",
    "print(\"Dataset structure:\")\n",
    "print(hf_dataset)\n",
    "\n",
    "# --- Load Tokenizer ---\n",
    "print(f\"\\nLoading tokenizer for {base_model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    if tokenizer.eos_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        print(\"Set pad_token to eos_token\")\n",
    "    else:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "        print(\"Added [PAD] as pad_token\")\n",
    "        # Will resize model embeddings in Cell 3\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# --- Tokenization and Label Masking Function ---\n",
    "# Prepare data for standard Trainer with Causal LM objective\n",
    "def tokenize_and_mask(examples):\n",
    "    # Combine input and output\n",
    "    input_prompts = [ex.strip() + \"\\n\" for ex in examples[\"input\"]]\n",
    "    outputs = [ex.strip() for ex in examples[\"output\"]]\n",
    "\n",
    "    processed_outputs = []\n",
    "    for out in outputs:\n",
    "        if not out.endswith(tokenizer.eos_token):\n",
    "            processed_outputs.append(out + tokenizer.eos_token)\n",
    "        else:\n",
    "            processed_outputs.append(out)\n",
    "\n",
    "    full_texts = [prompt + output for prompt, output in zip(input_prompts, processed_outputs)]\n",
    "\n",
    "    # Tokenize the full text (TRUNCATE, but DO NOT PAD here)\n",
    "    model_inputs = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=max_seq_length,\n",
    "        truncation=True,\n",
    "        padding=False, # *** CHANGED: Let collator handle padding ***\n",
    "        return_tensors=None, # *** CHANGED: Return lists ***\n",
    "    )\n",
    "\n",
    "    # Tokenize prompts separately to find their length for masking\n",
    "    # Important: Use return_tensors=None here too if not already\n",
    "    prompt_tokens_results = tokenizer(\n",
    "        input_prompts,\n",
    "        max_length=max_seq_length, # Truncate prompt if it's too long itself\n",
    "        truncation=True,\n",
    "        padding=False,\n",
    "        return_tensors=None,\n",
    "        add_special_tokens=False # Get length without special tokens if tokenizer adds them by default\n",
    "    )\n",
    "\n",
    "\n",
    "    # Create labels and mask\n",
    "    labels = []\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "\n",
    "    for i in range(len(model_inputs[\"input_ids\"])):\n",
    "        prompt_length = len(prompt_tokens_results['input_ids'][i])\n",
    "        full_token_ids = model_inputs[\"input_ids\"][i]\n",
    "\n",
    "        # Create labels: copy input_ids then mask\n",
    "        current_labels = list(full_token_ids) # Make a copy as a list\n",
    "        current_labels[:prompt_length] = [-100] * prompt_length # Mask prompt part\n",
    "\n",
    "        # Add the processed lists to our output lists\n",
    "        labels.append(current_labels)\n",
    "        input_ids_list.append(full_token_ids)\n",
    "        attention_mask_list.append(model_inputs[\"attention_mask\"][i]) # Keep original attention mask\n",
    "\n",
    "\n",
    "    # Return the dictionary expected by the dataset map function\n",
    "    return {\n",
    "        \"input_ids\": input_ids_list,\n",
    "        \"attention_mask\": attention_mask_list,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "# --- Apply Tokenization and Masking ---\n",
    "print(\"\\nTokenizing and masking dataset...\")\n",
    "tokenized_datasets = hf_dataset.map(\n",
    "    tokenize_and_mask,\n",
    "    batched=True, # Process in batches\n",
    "    remove_columns=hf_dataset[\"train\"].column_names # Remove original 'input', 'output'\n",
    ")\n",
    "\n",
    "print(\"Dataset structure after tokenization/masking:\")\n",
    "print(tokenized_datasets)\n",
    "print(\"\\nSample tokenized data point:\")\n",
    "idx = 0\n",
    "# Check lengths - they might vary now, which is expected before collation\n",
    "print(\"Input IDs length:\", len(tokenized_datasets[\"train\"][idx][\"input_ids\"]))\n",
    "print(\"Labels length:\", len(tokenized_datasets[\"train\"][idx][\"labels\"]))\n",
    "print(\"Input IDs sample:\", tokenized_datasets[\"train\"][idx][\"input_ids\"][:60])\n",
    "print(\"Labels sample:\", tokenized_datasets[\"train\"][idx][\"labels\"][:60])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55dbdcfe-c19f-4e23-a3e2-1815de300df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model 'Qwen/Qwen3-1.7B' in torch.bfloat16...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf3594103d547178cff06b39c6cdd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient checkpointing enabled.\n",
      "Identifying LoRA target modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
      "Applying PEFT LoRA adapter...\n",
      "Trainable parameters after LoRA application:\n",
      "trainable params: 17,432,576 || all params: 1,738,007,552 || trainable%: 1.0030\n",
      "\n",
      "Model device placement summary:\n",
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "# --- Load Base Model ---\n",
    "print(f\"Loading base model '{base_model_name}' in {torch_dtype}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "\n",
    "# --- Resize Embeddings if new token was added in Cell 2 ---\n",
    "if tokenizer.pad_token == '[PAD]':\n",
    "    print(\"Resizing model token embeddings for new PAD token...\")\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    print(\"Embeddings resized.\")\n",
    "# --- End Resize ---\n",
    "\n",
    "\n",
    "# --- Enable Gradient Checkpointing ---\n",
    "if gradient_checkpointing:\n",
    "    model.gradient_checkpointing_enable()\n",
    "    print(\"Gradient checkpointing enabled.\")\n",
    "\n",
    "\n",
    "# --- Configure LoRA ---\n",
    "print(f\"Identifying LoRA target modules: {target_modules}\")\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    r=lora_r,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=target_modules\n",
    ")\n",
    "\n",
    "print(\"Applying PEFT LoRA adapter...\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "print(\"Trainable parameters after LoRA application:\")\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "print(\"\\nModel device placement summary:\")\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3cdf4ce-0654-4787-a11a-3b874a7e5308",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1681/610757765.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer( # *** USING STANDARD TRAINER ***\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring Training Arguments...\n",
      "Initializing standard Trainer...\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1146' max='1146' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1146/1146 1:41:56, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.461300</td>\n",
       "      <td>0.424632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.430100</td>\n",
       "      <td>0.367419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.373600</td>\n",
       "      <td>0.337314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.277600</td>\n",
       "      <td>0.321070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.272400</td>\n",
       "      <td>0.309686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.255700</td>\n",
       "      <td>0.303475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.299505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.225100</td>\n",
       "      <td>0.300045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.231600</td>\n",
       "      <td>0.297692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.221300</td>\n",
       "      <td>0.297133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.211300</td>\n",
       "      <td>0.296829</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished.\n",
      "\n",
      "Saving final adapter model to 'qwen3-1.7b-linear-algebra-coder-lora-stdtrainer-final'...\n",
      "Final adapter model saved.\n",
      "\n",
      "Saving best adapter model (based on eval_loss) to 'qwen3-1.7b-linear-algebra-coder-lora-stdtrainer-best'...\n",
      "Best model saved to 'qwen3-1.7b-linear-algebra-coder-lora-stdtrainer-best'\n"
     ]
    }
   ],
   "source": [
    "# --- Set up Training Arguments ---\n",
    "print(\"Configuring Training Arguments...\")\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    optim=optim,\n",
    "    save_strategy=save_strategy,\n",
    "    save_steps=save_steps,\n",
    "    logging_dir=f\"{output_dir}/logs\",\n",
    "    logging_steps=logging_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=weight_decay,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    "    max_grad_norm=max_grad_norm,\n",
    "    max_steps=max_steps,\n",
    "    warmup_ratio=warmup_ratio,\n",
    "    group_by_length=group_by_length,\n",
    "    lr_scheduler_type=lr_scheduler_type,\n",
    "    report_to=report_to,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=eval_steps,\n",
    "    save_total_limit=save_total_limit,\n",
    "    load_best_model_at_end=load_best_model_at_end,\n",
    "    metric_for_best_model=metric_for_best_model,\n",
    "    greater_is_better=greater_is_better,\n",
    "    gradient_checkpointing=gradient_checkpointing,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "# --- Initialize STANDARD Trainer ---\n",
    "# Data collator is crucial for handling padding correctly\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "print(\"Initializing standard Trainer...\")\n",
    "trainer = Trainer( # *** USING STANDARD TRAINER ***\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=tokenized_datasets[\"train\"], # Pass the tokenized dataset\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # Use the ML collator\n",
    ")\n",
    "\n",
    "# --- Start Training ---\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "print(\"Training finished.\")\n",
    "\n",
    "# --- Save Final Adapter Model ---\n",
    "final_adapter_path = f\"{new_model_name}-final\"\n",
    "print(f\"\\nSaving final adapter model to '{final_adapter_path}'...\")\n",
    "model.save_pretrained(final_adapter_path) # Use model.save_pretrained for PEFT\n",
    "tokenizer.save_pretrained(final_adapter_path)\n",
    "print(\"Final adapter model saved.\")\n",
    "\n",
    "# --- Save Best Model Explicitly ---\n",
    "best_adapter_path = f\"{new_model_name}-best\"\n",
    "print(f\"\\nSaving best adapter model (based on eval_loss) to '{best_adapter_path}'...\")\n",
    "trainer.model.save_pretrained(best_adapter_path) # Save the currently loaded (should be best) model\n",
    "tokenizer.save_pretrained(best_adapter_path)\n",
    "print(f\"Best model saved to '{best_adapter_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6efd81f3-642f-4e40-b802-a1c04bb2d545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Setting up for Inference ---\n",
      "--- Using GPU ---\n",
      "Loading adapter from: qwen1.8b-lora-stdtrainer-results/checkpoint-1146\n",
      "Loading base model 'Qwen/Qwen3-1.7B' for inference...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "974c91156b5f4f3ba3a009153bade9c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned adapter from 'qwen1.8b-lora-stdtrainer-results/checkpoint-1146'...\n",
      "Inference model ready.\n",
      "\n",
      "Input Prompt:\n",
      "You are a helpful coding assistant. Given a linear algebra problem, provide only the Python code solution using numpy, scipy, or sympy. Do not include any explanations, comments, or introductory text.\n",
      "\n",
      "### Problem:\n",
      "Let A = [[1, 2], [3, 4], [5, 6]]. Perform Singular Value Decomposition (SVD) on matrix A and find its singular values.\n",
      "\n",
      "### Python Code Solution:\n",
      "\n",
      "Generating code...\n",
      "Generation complete.\n",
      "INFO: Detected subsequent import/from, truncating after first block.\n",
      "------------------------------\n",
      "Generated Code Output (Extracted):\n",
      "import numpy as np\n",
      "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
      "U, S, Vt = np.linalg.svd(A)\n",
      "print('Singular values:', S)  # Output: [7.748... 1.527...] (approximate)  # noqa: E501\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel\n",
    "import numpy as np\n",
    "import re # Import regular expressions\n",
    "\n",
    "# --- Inference Setup ---\n",
    "print(\"\\n--- Setting up for Inference ---\")\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"--- Using GPU ---\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"--- Using CPU ---\")\n",
    "adapter_to_load = f\"qwen1.8b-lora-stdtrainer-results/checkpoint-1146\" # Or final_adapter_path\n",
    "print(f\"Loading adapter from: {adapter_to_load}\")\n",
    "\n",
    "# Load the base model\n",
    "print(f\"Loading base model '{base_model_name}' for inference...\")\n",
    "base_model_for_inf = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    torch_dtype=torch_dtype,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Load the PEFT adapter\n",
    "print(f\"Loading fine-tuned adapter from '{adapter_to_load}'...\")\n",
    "model_inf = PeftModel.from_pretrained(base_model_for_inf, adapter_to_load)\n",
    "model_inf = model_inf.eval()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_inf = AutoTokenizer.from_pretrained(adapter_to_load)\n",
    "if tokenizer_inf.pad_token is None:\n",
    "    tokenizer_inf.pad_token = tokenizer_inf.eos_token\n",
    "\n",
    "# Define EOS token ID and string\n",
    "eos_token_id = tokenizer_inf.eos_token_id\n",
    "eos_token_str = tokenizer_inf.eos_token\n",
    "if eos_token_str is None:\n",
    "    eos_token_str = \"<|endoftext|>\" # Adjust if needed\n",
    "\n",
    "print(\"Inference model ready.\")\n",
    "\n",
    "prompt_template = \"\"\"You are a helpful coding assistant. Given a linear algebra problem, provide only the Python code solution using numpy, scipy, or sympy. Do not include any explanations, comments, or introductory text.\n",
    "\n",
    "### Problem:\n",
    "{problem_text}\n",
    "\n",
    "### Python Code Solution:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# --- Generation Function with Aggressive Cut-off ---\n",
    "def generate_and_extract_first_code_block(prompt_text, max_new=150, temp=0.1):\n",
    "    full_prompt = prompt_template.format(problem_text=prompt_text)\n",
    "    print(f\"\\nInput Prompt:\\n{full_prompt}\")\n",
    "\n",
    "    inputs = tokenizer_inf(full_prompt, return_tensors=\"pt\", add_special_tokens=True).to(device)\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    generation_config = GenerationConfig(\n",
    "        max_new_tokens=max_new,\n",
    "        temperature=temp,\n",
    "        do_sample=False,\n",
    "        eos_token_id=eos_token_id,\n",
    "        pad_token_id=eos_token_id,\n",
    "    )\n",
    "\n",
    "    print(\"Generating code...\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model_inf.generate(**inputs, generation_config=generation_config)\n",
    "    print(\"Generation complete.\")\n",
    "\n",
    "    # Decode only the generated part (keep special tokens for initial EOS check)\n",
    "    generated_token_ids = outputs[0][input_length:]\n",
    "    generated_text_raw = tokenizer_inf.decode(generated_token_ids, skip_special_tokens=False)\n",
    "\n",
    "    # --- Aggressive First Block Extraction Logic ---\n",
    "    # 1. Find the primary EOS stop first\n",
    "    eos_pos = generated_text_raw.find(eos_token_str)\n",
    "    if eos_pos != -1:\n",
    "        code_candidate = generated_text_raw[:eos_pos].rstrip()\n",
    "    else:\n",
    "        code_candidate = generated_text_raw.rstrip() # Use raw if EOS not found\n",
    "\n",
    "    # 2. Basic stripping and initial fence removal\n",
    "    code_candidate = code_candidate.strip()\n",
    "    if code_candidate.startswith(\"```python\"):\n",
    "         code_candidate = code_candidate[len(\"```python\"):].lstrip()\n",
    "    elif code_candidate.startswith(\"```\"):\n",
    "         code_candidate = code_candidate[3:].lstrip()\n",
    "\n",
    "    # 3. Find the *first* occurrence of `print(` or the last line if no print\n",
    "    lines = code_candidate.splitlines()\n",
    "    end_of_first_block_char_index = len(code_candidate) # Default to end\n",
    "    found_print = False\n",
    "    current_char_count = 0\n",
    "\n",
    "    for i, line in enumerate(lines):\n",
    "        current_char_count += len(line) + 1 # Account for newline\n",
    "        if line.strip().startswith(\"print(\"):\n",
    "            end_of_first_block_char_index = current_char_count\n",
    "            found_print = True\n",
    "            break # Stop after the first print line\n",
    "\n",
    "    # If no print found, assume the whole thing is the block for now\n",
    "    if not found_print:\n",
    "         end_of_first_block_char_index = len(code_candidate)\n",
    "\n",
    "    # 4. Check for subsequent 'import' or 'from' *immediately* after this block\n",
    "    text_after_block = code_candidate[end_of_first_block_char_index:]\n",
    "    import_match = re.search(r\"^\\s*(import|from)\\s\", text_after_block, re.MULTILINE)\n",
    "    \n",
    "    # 5. Truncate *before* the subsequent import/from if found\n",
    "    if import_match:\n",
    "        # Cut off at the end of the first block, right before the offending import\n",
    "        extracted_code = code_candidate[:end_of_first_block_char_index].strip()\n",
    "        print(\"INFO: Detected subsequent import/from, truncating after first block.\")\n",
    "    else:\n",
    "        # If no subsequent import found, assume the block up to print/end is correct\n",
    "        extracted_code = code_candidate[:end_of_first_block_char_index].strip()\n",
    "    lines = extracted_code.splitlines()\n",
    "    if lines and lines[0].strip() == \"py\":\n",
    "        extracted_code = \"\\n\".join(lines[1:]).lstrip()\n",
    "    # 6. Final cleanup for trailing fences\n",
    "    if extracted_code.endswith(\"```\"):\n",
    "         extracted_code = extracted_code[:-3].strip()\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Generated Code Output (Extracted):\\n{extracted_code}\")\n",
    "    print(\"-\" * 30)\n",
    "    return extracted_code\n",
    "\n",
    "\n",
    "# --- Test Prompts ---\n",
    "\n",
    "problem_1 =  \"Let A = [[1, 2], [3, 4], [5, 6]]. Perform Singular Value Decomposition (SVD) on matrix A and find its singular values.\"\n",
    "\n",
    "code1 = generate_and_extract_first_code_block(problem_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325aaacb-f258-4f5a-a5b2-6966363cfd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Singular values: [9.52551809 0.51430058]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])\n",
    "U, S, Vt = np.linalg.svd(A)\n",
    "print('Singular values:', S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207fffe-fdd4-4d34-81eb-28790442f119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
